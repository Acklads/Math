from docx import Document
import re
import os
import json
from datetime import datetime

class MathModelingAnalyzer:
    """
    æ•°å­¦å»ºæ¨¡ç«èµ›é¢˜ç›®åˆ†æå™¨
    ç”¨äºè¯»å–Wordæ–‡æ¡£ä¸­çš„é¢˜ç›®å†…å®¹ï¼Œåˆ†æé¢˜ç›®ç»“æ„ï¼Œå¹¶ç”Ÿæˆé¡¹ç›®ç®¡ç†ä¿¡æ¯
    """
    
    def __init__(self, docx_path):
        self.docx_path = docx_path
        self.content = ""
        self.problems = {}
        self.analysis_result = {}
        
    def read_document(self):
        """
        è¯»å–Wordæ–‡æ¡£å†…å®¹
        """
        try:
            doc = Document(self.docx_path)
            full_text = []
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            self.content = '\n'.join(full_text)
            print(f"âœ“ æˆåŠŸè¯»å–æ–‡æ¡£: {os.path.basename(self.docx_path)}")
            return True
            
        except Exception as e:
            print(f"âœ— è¯»å–æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False
    
    def analyze_problems(self):
        """
        åˆ†æé¢˜ç›®ä¸­çš„é—®é¢˜ç»“æ„
        """
        if not self.content:
            print("âœ— è¯·å…ˆè¯»å–æ–‡æ¡£å†…å®¹")
            return False
        
        # æŸ¥æ‰¾é—®é¢˜æ ‡è¯†
        problem_patterns = [
            r'é—®é¢˜[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1-9][:ï¼š]?',
            r'Problem\s*[1-9][:ï¼š]?',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1-9][ä¸ªé¢˜]é—®é¢˜[:ï¼š]?',
            r'\([1-9]\)',
            r'[1-9]\.',
        ]
        
        lines = self.content.split('\n')
        current_problem = None
        
        for i, line in enumerate(lines):
            # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜æ ‡é¢˜
            for pattern in problem_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    # æå–é—®é¢˜ç¼–å·
                    problem_num = self._extract_problem_number(line)
                    if problem_num and problem_num <= 3:  # é€šå¸¸æ•°å­¦å»ºæ¨¡æœ‰3ä¸ªé—®é¢˜
                        current_problem = problem_num
                        self.problems[current_problem] = {
                            'title': line,
                            'content': [],
                            'line_start': i
                        }
                        print(f"âœ“ å‘ç°é—®é¢˜{current_problem}: {line[:50]}...")
                    break
            else:
                # å¦‚æœå½“å‰è¡Œä¸æ˜¯é—®é¢˜æ ‡é¢˜ï¼Œä¸”æœ‰å½“å‰é—®é¢˜ï¼Œåˆ™æ·»åŠ åˆ°å†…å®¹ä¸­
                if current_problem and line.strip():
                    self.problems[current_problem]['content'].append(line)
        
        return len(self.problems) > 0
    
    def _extract_problem_number(self, text):
        """
        ä»æ–‡æœ¬ä¸­æå–é—®é¢˜ç¼–å·
        """
        # ä¸­æ–‡æ•°å­—æ˜ å°„
        chinese_nums = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5}
        
        # æŸ¥æ‰¾é˜¿æ‹‰ä¼¯æ•°å­—
        arabic_match = re.search(r'[1-9]', text)
        if arabic_match:
            return int(arabic_match.group())
        
        # æŸ¥æ‰¾ä¸­æ–‡æ•°å­—
        for chinese, num in chinese_nums.items():
            if chinese in text:
                return num
        
        return None
    
    def generate_analysis_report(self):
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        """
        self.analysis_result = {
            'document_info': {
                'filename': os.path.basename(self.docx_path),
                'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_lines': len(self.content.split('\n')),
                'total_characters': len(self.content)
            },
            'problems_found': len(self.problems),
            'problems_detail': {}
        }
        
        for prob_num, prob_data in self.problems.items():
            content_text = '\n'.join(prob_data['content'])
            self.analysis_result['problems_detail'][f'é—®é¢˜{prob_num}'] = {
                'title': prob_data['title'],
                'content_length': len(content_text),
                'content_lines': len(prob_data['content']),
                'keywords': self._extract_keywords(content_text),
                'suggested_methods': self._suggest_methods(content_text)
            }
        
        return self.analysis_result
    
    def _extract_keywords(self, text):
        """
        æå–å…³é”®è¯
        """
        # æ•°å­¦å»ºæ¨¡å¸¸è§å…³é”®è¯
        keywords = [
            'ä¼˜åŒ–', 'å»ºæ¨¡', 'é¢„æµ‹', 'åˆ†æ', 'ç®—æ³•', 'æ¨¡å‹', 'æ•°æ®', 'ç»Ÿè®¡',
            'å›å½’', 'èšç±»', 'åˆ†ç±»', 'ç¥ç»ç½‘ç»œ', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ',
            'çº¿æ€§è§„åˆ’', 'éçº¿æ€§', 'åŠ¨æ€è§„åˆ’', 'é—ä¼ ç®—æ³•', 'æ¨¡æ‹Ÿé€€ç«',
            'è’™ç‰¹å¡æ´›', 'æ—¶é—´åºåˆ—', 'æ¦‚ç‡', 'éšæœº', 'ä»¿çœŸ', 'è¯„ä»·',
            'å†³ç­–', 'å¤šç›®æ ‡', 'çº¦æŸ', 'æœ€ä¼˜åŒ–', 'æ±‚è§£'
        ]
        
        found_keywords = []
        for keyword in keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        return found_keywords[:10]  # è¿”å›å‰10ä¸ªå…³é”®è¯
    
    def _suggest_methods(self, text):
        """
        æ ¹æ®å†…å®¹å»ºè®®å¯èƒ½çš„è§£å†³æ–¹æ³•
        """
        suggestions = []
        
        if any(word in text for word in ['ä¼˜åŒ–', 'æœ€ä¼˜', 'æœ€å¤§', 'æœ€å°']):
            suggestions.append('æ•°å­¦ä¼˜åŒ–æ–¹æ³•')
        
        if any(word in text for word in ['é¢„æµ‹', 'é¢„æŠ¥', 'è¶‹åŠ¿']):
            suggestions.append('æ—¶é—´åºåˆ—åˆ†æ')
        
        if any(word in text for word in ['åˆ†ç±»', 'èšç±»', 'è¯†åˆ«']):
            suggestions.append('æœºå™¨å­¦ä¹ æ–¹æ³•')
        
        if any(word in text for word in ['è¯„ä»·', 'è¯„ä¼°', 'æ’åº']):
            suggestions.append('å¤šæŒ‡æ ‡è¯„ä»·æ–¹æ³•')
        
        if any(word in text for word in ['ç½‘ç»œ', 'å›¾', 'è·¯å¾„']):
            suggestions.append('å›¾è®ºæ–¹æ³•')
        
        if any(word in text for word in ['ä»¿çœŸ', 'æ¨¡æ‹Ÿ', 'éšæœº']):
            suggestions.append('è’™ç‰¹å¡æ´›ä»¿çœŸ')
        
        return suggestions if suggestions else ['éœ€è¦è¿›ä¸€æ­¥åˆ†æ']
    
    def save_analysis(self, output_file='é¢˜ç›®åˆ†æç»“æœ.json'):
        """
        ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_result, f, ensure_ascii=False, indent=2)
            print(f"âœ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            return True
        except Exception as e:
            print(f"âœ— ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    def print_summary(self):
        """
        æ‰“å°åˆ†ææ‘˜è¦
        """
        print("\n" + "="*60)
        print("ğŸ“Š æ•°å­¦å»ºæ¨¡é¢˜ç›®åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if self.analysis_result:
            doc_info = self.analysis_result['document_info']
            print(f"ğŸ“„ æ–‡æ¡£åç§°: {doc_info['filename']}")
            print(f"â° åˆ†ææ—¶é—´: {doc_info['analysis_time']}")
            print(f"ğŸ“ æ€»è¡Œæ•°: {doc_info['total_lines']}")
            print(f"ğŸ”¤ æ€»å­—ç¬¦æ•°: {doc_info['total_characters']}")
            print(f"â“ å‘ç°é—®é¢˜æ•°: {self.analysis_result['problems_found']}")
            
            print("\nğŸ“‹ é—®é¢˜è¯¦æƒ…:")
            for prob_name, prob_detail in self.analysis_result['problems_detail'].items():
                print(f"\n  {prob_name}:")
                print(f"    æ ‡é¢˜: {prob_detail['title']}")
                print(f"    å†…å®¹é•¿åº¦: {prob_detail['content_length']} å­—ç¬¦")
                print(f"    å…³é”®è¯: {', '.join(prob_detail['keywords'])}")
                print(f"    å»ºè®®æ–¹æ³•: {', '.join(prob_detail['suggested_methods'])}")
        
        print("\n" + "="*60)

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¯ æ•°å­¦å»ºæ¨¡ç«èµ›é¢˜ç›®åˆ†æå™¨")
    print("=" * 40)
    
    # æŸ¥æ‰¾é¡¹ç›®ä¸­çš„Wordæ–‡æ¡£
    docx_files = [f for f in os.listdir('.') if f.endswith('.docx')]
    
    if not docx_files:
        print("âŒ æœªæ‰¾åˆ°Wordæ–‡æ¡£æ–‡ä»¶")
        print("è¯·å°†é¢˜ç›®Wordæ–‡æ¡£æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹")
        return
    
    # å¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæˆ–è®©ç”¨æˆ·é€‰æ‹©
    if len(docx_files) == 1:
        selected_file = docx_files[0]
        print(f"ğŸ“„ æ‰¾åˆ°æ–‡æ¡£: {selected_file}")
    else:
        print("ğŸ“„ æ‰¾åˆ°å¤šä¸ªWordæ–‡æ¡£:")
        for i, file in enumerate(docx_files, 1):
            print(f"  {i}. {file}")
        
        try:
            choice = int(input("è¯·é€‰æ‹©è¦åˆ†æçš„æ–‡æ¡£ç¼–å·: ")) - 1
            selected_file = docx_files[choice]
        except (ValueError, IndexError):
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£")
            selected_file = docx_files[0]
    
    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyzer = MathModelingAnalyzer(selected_file)
    
    if analyzer.read_document():
        if analyzer.analyze_problems():
            analyzer.generate_analysis_report()
            analyzer.print_summary()
            analyzer.save_analysis()
            
            print("\nğŸ‰ åˆ†æå®Œæˆï¼")
            print("ğŸ’¡ å»ºè®®æ¥ä¸‹æ¥çš„æ­¥éª¤:")
            print("  1. æŸ¥çœ‹ç”Ÿæˆçš„åˆ†ææŠ¥å‘Šæ–‡ä»¶")
            print("  2. æ ¹æ®é—®é¢˜ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ç®—æ³•å’Œå·¥å…·")
            print("  3. åœ¨å¯¹åº”çš„é—®é¢˜æ–‡ä»¶å¤¹ä¸­å¼€å§‹ç¼–ç ")
            print("  4. å°†å¯è§†åŒ–ç»“æœä¿å­˜åˆ°å¯¹åº”çš„ç»“æœæ–‡ä»¶å¤¹")
        else:
            print("âŒ æœªèƒ½è¯†åˆ«å‡ºé—®é¢˜ç»“æ„ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£æ ¼å¼")
    else:
        print("âŒ æ–‡æ¡£è¯»å–å¤±è´¥")

if __name__ == "__main__":
    main()